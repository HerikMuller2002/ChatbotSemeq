{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d4e3b03",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f6fb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "from re import sub\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from services.database import *\n",
    "import language_tool_python\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bab1731f",
   "metadata": {},
   "source": [
    "### treinar o modelo de classificação para realizar a correção ortografica de palavras especificas\n",
    "#### . usar api openai para rotular lista de palavras para treinar o modelo\n",
    "### usar o modelo treinado para corrigir o texto ante do spellchecker\n",
    "#### - excluir as palavras corrigidas antes do spellchecker e adiciona-lo posteriormente"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "654080ee",
   "metadata": {},
   "source": [
    "## Pre-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07843592",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell_pt = SpellChecker(language='pt')\n",
    "spell_en = SpellChecker(language='en')\n",
    "def preprocess_correcao(text):\n",
    "    list_input = word_tokenize(text)\n",
    "    list_text = []\n",
    "    for i in list_input:\n",
    "        correcao = spell_pt.correction(i)\n",
    "        if correcao == None:\n",
    "            correcao = spell_en.correction(i)\n",
    "        list_text.append(correcao)\n",
    "    text = ' '.join(list_text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_semantic(frase):\n",
    "    tool = language_tool_python.LanguageTool('pt')\n",
    "    matches = tool.check(frase)\n",
    "    for i in matches:\n",
    "        frase = frase[:i.offset] + i.replacements[0] + frase[i.offset+i.errorLength:]\n",
    "    tool.close()\n",
    "    return frase\n",
    "\n",
    "\n",
    "def preprocess_stem(text):\n",
    "    stemmer = SnowballStemmer(\"portuguese\")\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = [stemmer.stem(token) for token in tokens]\n",
    "    text = ' '.join([str(element) for element in stems])\n",
    "    return text\n",
    "\n",
    "\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "def key_words(text):\n",
    "    doc = nlp(text)\n",
    "    lista = ['VERB','NOUN','ADJ','PROPN']\n",
    "    key_tokens = [(token.text, token.pos_) for token in doc if token.pos_ in lista]\n",
    "    text = ' '.join([str(element[0]) for element in key_tokens])\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_input(text):\n",
    "    text = preprocess_correcao(text)\n",
    "    text = preprocess_semantic(text)\n",
    "    print(text)\n",
    "    text = key_words(text)\n",
    "    text = preprocess_stem(text)\n",
    "    text = sub(r\"[#&+-./↔>?@[^_`’|}~]+\", ' ',text)\n",
    "    text = sub(r\"]\", ' ',text)\n",
    "    text = text.lower().strip()\n",
    "    # tirar pontuações, acentos e espaços extras\n",
    "    text = sub('[áàãâä]', 'a', sub('[éèêë]', 'e', sub('[íìîï]', 'i', sub('[óòõôö]', 'o', sub('[úùûü]', 'u', text)))))\n",
    "    # tirar espaços em branco\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44908e15",
   "metadata": {},
   "source": [
    "## testes do pre-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572cdf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def teste(texto):\n",
    "    b = preprocess_input(texto)\n",
    "    print(b)\n",
    "    print()\n",
    "\n",
    "lista = [\n",
    "    # \"O coletor não liga, eu tenti pressionar o butão de ligar, mas nada acontece.\",\n",
    "    # \"O touch do coletor está travadu, não responde as meus comandos.\",\n",
    "    # \"Recebi um alerta de ruido ou ausencia de sinal, isso afeta a colleta de dados.\",\n",
    "    \"Há um alerta de overloaad no mte, o equipamentu está enfrentado uma sobrecarga de dados.\"\n",
    "]\n",
    "\n",
    "for i in lista:\n",
    "    teste(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4bb20dfe",
   "metadata": {},
   "source": [
    "# rotulando os dados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f89d7c2",
   "metadata": {},
   "source": [
    "### Api openAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a2a54b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def rotulo(txt):\n",
    "  req = f\"Preciso criar um modelo que classifica uma palavra errada para a certa, exemplo 'mtee' seria corrigido para 'mte'. para isso irei te dar uma lista de palavras certas e preciso que vc me retorne uma lista python de pelo menos 7 palavras com a ortografia errada para cada uma delas. me retorne a resposta no seguinte formato: em um dicionário python onde a chave seria a palavra certa e o valor seria uma lista python com as palavras erradas. aqui esão as palavras: {txt}\"\n",
    "\n",
    "  completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "      {\"role\": \"user\", \"content\": req}\n",
    "    ]\n",
    "  )\n",
    "  res = completion.choices[0].message.content\n",
    "  return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e81c94ee",
   "metadata": {},
   "source": [
    "### Criando o df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4c88839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(r'database\\troubleshooting.xlsx')\n",
    "lista = []\n",
    "for idx in df['description']:\n",
    "    lista.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "255db772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "for i in lista:\n",
    "    try:\n",
    "        response = rotulo(i)\n",
    "    except:\n",
    "        time.sleep(60)\n",
    "        response = rotulo(i)\n",
    "        continue\n",
    "    with open('response.json','r+',encoding='utf-8') as f:\n",
    "        log = json.load(f)\n",
    "        log.append(response)\n",
    "        f.seek(0)\n",
    "        json.dump(log, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030c930d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
